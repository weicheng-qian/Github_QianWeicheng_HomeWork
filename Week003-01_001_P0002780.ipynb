{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbf36f77-f31b-4d76-b8aa-677ed6ad7e80",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/root/anaconda3/envs/AliOpenAPI/lib/python3.11/site-packages/awq_inference_engine.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c104cuda9SetDeviceEi",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mawq\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoAWQForCausalLM\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# 检查 PyTorch 和 CUDA\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPyTorch 版本: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/AliOpenAPI/lib/python3.11/site-packages/awq/__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m __version__ = \u001b[33m\"\u001b[39m\u001b[33m0.1.6\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mawq\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoAWQForCausalLM\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/AliOpenAPI/lib/python3.11/site-packages/awq/models/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmpt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MptAWQForCausalLM\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllama\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LlamaAWQForCausalLM\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mopt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OptAWQForCausalLM\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/AliOpenAPI/lib/python3.11/site-packages/awq/models/mpt.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseAWQForCausalLM\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmpt\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_mpt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MptBlock \u001b[38;5;28;01mas\u001b[39;00m OldMptBlock, MptForCausalLM\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMptAWQForCausalLM\u001b[39;00m(BaseAWQForCausalLM):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/AliOpenAPI/lib/python3.11/site-packages/awq/models/base.py:12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mawq\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodules\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mact\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ScaledActivation\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m snapshot_download\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mawq\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquantize\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquantizer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AwqQuantizer\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m shard_checkpoint\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mawq\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodules\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinear\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WQLinear_GEMM, WQLinear_GEMV\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/AliOpenAPI/lib/python3.11/site-packages/awq/quantize/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mw8a8_linear\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01msmooth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/AliOpenAPI/lib/python3.11/site-packages/awq/quantize/w8a8_linear.py:6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optional, Union\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Parameter\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mawq_inference_engine\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgc\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: /root/anaconda3/envs/AliOpenAPI/lib/python3.11/site-packages/awq_inference_engine.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c104cuda9SetDeviceEi"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from awq import AutoAWQForCausalLM\n",
    "\n",
    "# 检查 PyTorch 和 CUDA\n",
    "print(f\"PyTorch 版本: {torch.__version__}\")\n",
    "print(f\"CUDA 可用: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA 版本: {torch.version.cuda}\")\n",
    "    print(f\"GPU 设备: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA 不可用，将使用 CPU\")\n",
    "\n",
    "# 检查 transformers\n",
    "try:\n",
    "    model_name = \"facebook/opt-1.3b\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    print(\"transformers 库验证成功\")\n",
    "except Exception as e:\n",
    "    print(f\"transformers 库验证失败: {str(e)}\")\n",
    "\n",
    "# 检查 AWQ\n",
    "try:\n",
    "    # 使用一个小型的 AWQ 量化模型进行测试\n",
    "    model_name_or_path = \"TheBloke/Llama-2-7B-Chat-AWQ\"\n",
    "    model = AutoAWQForCausalLM.from_quantized(\n",
    "        model_name_or_path,\n",
    "        fuse_layers=True,\n",
    "        trust_remote_code=False,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=False)\n",
    "    print(\"AWQ 库验证成功\")\n",
    "except Exception as e:\n",
    "    print(f\"AWQ 库验证失败: {str(e)}\")\n",
    "\n",
    "# 简单推理测试\n",
    "if torch.cuda.is_available() and 'model' in locals():\n",
    "    try:\n",
    "        prompt = \"Hello, world!\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\n",
    "        outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "        print(\"推理测试结果:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "    except Exception as e:\n",
    "        print(f\"推理测试失败: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b12933f8-c320-4c51-8ce7-e5cf8f7b9b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始量化模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/AliOpenAPI/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/root/anaconda3/envs/AliOpenAPI/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/root/anaconda3/envs/AliOpenAPI/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float16)\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.          \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected gptqmodel and auto-gptq, will use gptqmodel\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8b7ac376bc4b3ba147cd3ddb6ef784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Detected gptqmodel and auto-gptq, will use gptqmodel\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c961aa67c046b0bc8aab1e59230f5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b2e01a35934094bf963b715b37dc4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:optimum.gptq.quantizer:Detected gptqmodel and auto-gptq, will use gptqmodel. The auto_gptq will be deprecated in the future.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cdc1e1dc06d4eeaa2cdd0ea4867364c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c874ad9421e4c57b2c5cea7b567b2cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee1289db00f4444a37decdb97752bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a102fd1d7cca4b608bb6c4960979ecaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5111b6212ad4a6d87281e78bb14ff83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "867ec10493004c92b48b9ed57937769f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd479ebaf1554a02963bdd7840a956fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "701eb43ce5dd4013a4ca08f4126f4450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb054991f4b44d078c046d20b1bf9117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea42227b0154ab8b89deba53a7917f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61444785155941d79f1a2f1dd5576d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be08ed9fbc794de7a1685a81f02e67e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing model.decoder.layers blocks :   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 1/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbba63e2712f4177af2dac163c77115b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 1/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 1/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 1/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 1/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 1/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 1/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 2/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a44abaf520947a493e6928282b9111a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 2/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 2/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 2/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 2/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 2/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 2/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 3/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b956de371c4eb892bbd52dfab13aae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 3/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 3/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 3/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 3/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 3/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 3/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 4/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e10866def342c89e0f399b4484e146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 4/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 4/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 4/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 4/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 4/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 4/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 5/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c960fff21604806a790234518dc4bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 5/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 5/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 5/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 5/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 5/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 5/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 6/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d64c5f218f984a0fac0b02109c52d3c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 6/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 6/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 6/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 6/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 6/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 6/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 7/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2822f8177a6f42d2bf11894d0abe9d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 7/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 7/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 7/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 7/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 7/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 7/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 8/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c26a4ab5dfc4eee8bacea5b8972a95c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 8/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 8/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 8/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 8/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 8/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 8/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 9/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8685fc5b4cd4e45b556f08954b2ed33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 9/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 9/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 9/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 9/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 9/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 9/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 10/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bdfd7cf63d8450c9ec139ac846d56e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 10/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 10/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 10/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 10/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 10/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 10/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 11/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9490ea2051f94a2692543cd75e9e5df6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 11/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 11/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 11/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 11/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 11/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 11/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 12/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df94c6c7449f45549d92c698ddbbd063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 12/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 12/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 12/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 12/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 12/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 12/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 13/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f04ae2f20944b78b8f22321f0af3196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 13/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 13/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 13/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 13/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 13/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 13/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 14/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39deb26ce2b4cbe9d06a4ef2df43c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 14/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 14/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 14/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 14/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 14/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 14/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 15/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "becb161416a2479aa99dfadc43c03b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 15/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 15/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 15/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 15/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 15/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 15/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 16/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf714c1d62ec452d883322f63609e917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 16/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 16/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 16/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 16/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 16/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 16/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 17/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2875970ebe2e4e4890d66e75064e4790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 17/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 17/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 17/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 17/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 17/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 17/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 18/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4658ebe3c66d4473a7e9f761c79861c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 18/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 18/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 18/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 18/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 18/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 18/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 19/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457783fc3ba74d6aa36c5b65389a53f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 19/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 19/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 19/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 19/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 19/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 19/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 20/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "931457a52cc6403aa402bebd93bea783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 20/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 20/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 20/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 20/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 20/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 20/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 21/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68963d60b2eb4b618d01f998123d4547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 21/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 21/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 21/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 21/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 21/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 21/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 22/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d04e57ba2ef41c1919842bc517ff50d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 22/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 22/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 22/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 22/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 22/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 22/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 23/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c6d6cbfd884ea486f94c8d4be2318a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 23/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 23/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 23/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 23/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 23/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 23/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 24/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68b899177cee4db5832ed4b38af95ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 24/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 24/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 24/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 24/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 24/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 24/24...\n",
      "INFO:optimum.gptq.quantizer:Packing model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TritonV2QuantLinear`   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.0.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.0.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.0.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.0.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.0.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.0.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.1.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.1.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.1.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.1.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.1.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.1.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.2.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.2.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.2.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.2.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.2.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.2.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.3.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.3.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.3.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.3.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.3.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.3.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.4.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.4.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.4.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.4.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.4.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.4.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.5.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.5.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.5.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.5.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.5.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.5.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.6.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.6.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.6.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.6.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.6.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.6.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.7.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.7.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.7.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.7.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.7.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.7.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.8.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.8.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.8.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.8.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.8.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.8.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.9.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.9.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.9.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.9.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.9.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.9.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.10.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.10.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.10.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.10.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.10.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.10.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.11.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.11.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.11.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.11.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.11.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.11.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.12.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.12.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.12.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.12.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.12.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.12.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.13.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.13.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.13.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.13.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.13.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.13.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.14.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.14.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.14.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.14.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.14.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.14.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.15.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.15.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.15.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.15.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.15.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.15.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.16.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.16.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.16.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.16.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.16.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.16.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.17.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.17.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.17.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.17.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.17.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.17.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.18.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.18.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.18.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.18.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.18.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.18.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.19.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.19.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.19.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.19.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.19.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.19.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.20.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.20.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.20.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.20.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.20.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.20.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.21.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.21.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.21.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.21.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.21.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.21.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.22.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.22.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.22.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.22.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.22.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.22.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.23.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.23.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.23.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.23.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.23.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.23.fc2\n",
      "INFO:optimum.gptq.quantizer:Model packed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Optimize: `TritonV2QuantLinear` compilation triggered.                   \n",
      "\n",
      "检查量化是否成功：\n",
      "量化权重(qweight)是否存在: False\n",
      "量化零点(qzeros)数据类型: 未量化\n",
      "\n",
      "量化模型已保存至: opt-1.3b-gptq-4bit\n",
      "\n",
      "默认数据集量化模型生成结果:\n",
      "Merry Christmas! I'm glad to see you're still around.\n",
      "I'm still around, just not on reddit.\n",
      "\n",
      "使用自定义数据集进行量化...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected gptqmodel and auto-gptq, will use gptqmodel\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Detected gptqmodel and auto-gptq, will use gptqmodel\n",
      "WARNING:optimum.gptq.quantizer:Detected gptqmodel and auto-gptq, will use gptqmodel. The auto_gptq will be deprecated in the future.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee812fba2fb34ccd8ba3e84fb3d6134d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing model.decoder.layers blocks :   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 1/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb8c1349428468480afb50b9bb7c5f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 1/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 1/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 1/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 1/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 1/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 1/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 2/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1853803fa1d448d28eaffa4bd46061f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 2/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 2/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 2/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 2/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 2/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 2/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 3/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1cf82e89ae7437fb31a192f56065e3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 3/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 3/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 3/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 3/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 3/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 3/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 4/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "085da6d37dbb45d3913645e2854b56b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 4/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 4/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 4/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 4/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 4/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 4/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 5/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3009bdce14466ba286c1e7a78800b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 5/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 5/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 5/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 5/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 5/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 5/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 6/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9b93d0e0bf4ee69c12e41995047f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 6/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 6/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 6/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 6/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 6/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 6/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 7/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13706e3295f94ac98b782ac817982e1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 7/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 7/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 7/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 7/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 7/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 7/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 8/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91edcedecd564b65ae12a4bb2269e8d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 8/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 8/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 8/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 8/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 8/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 8/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 9/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "871c3721cfd54a9abf98f794b1339ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 9/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 9/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 9/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 9/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 9/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 9/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 10/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cfb333d0f38453089521b25dff65901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 10/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 10/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 10/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 10/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 10/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 10/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 11/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d39383723c4644e789ad0a9e0d61a131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 11/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 11/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 11/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 11/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 11/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 11/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 12/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6460f6f8f95b476fbf7e62a754d88a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 12/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 12/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 12/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 12/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 12/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 12/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 13/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f904177ec04592b38b80e8a9a4efdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 13/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 13/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 13/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 13/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 13/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 13/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 14/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51c534659dd4eb384a3de5635716676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 14/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 14/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 14/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 14/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 14/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 14/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 15/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49387616f1f2468c889795232a2196a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 15/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 15/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 15/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 15/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 15/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 15/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 16/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "684b4e0151134fd5b377b3b83fe359e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 16/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 16/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 16/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 16/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 16/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 16/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 17/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59f734ef0a6e458b8586ce363cbf514c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 17/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 17/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 17/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 17/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 17/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 17/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 18/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3c759ac7ca6420f9b5c21924e3e12b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 18/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 18/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 18/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 18/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 18/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 18/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 19/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ddd763010a6447696fe768cfeb1436f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 19/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 19/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 19/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 19/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 19/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 19/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 20/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85cbf3aa766742cf941f9704a25e3cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 20/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 20/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 20/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 20/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 20/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 20/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 21/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d03ffd54aa34989aaee14ae371359c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 21/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 21/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 21/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 21/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 21/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 21/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 22/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e8fbf576a74ae49138d13124ca3d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 22/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 22/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 22/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 22/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 22/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 22/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 23/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae311f981051438cba9524df228a056d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 23/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 23/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 23/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 23/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 23/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 23/24...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 24/24\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68ec5d3d54e548bfaad37d152ab08c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 24/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 24/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 24/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 24/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 24/24...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 24/24...\n",
      "INFO:optimum.gptq.quantizer:Packing model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TritonV2QuantLinear`   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:model.decoder.layers.0.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.0.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.0.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.0.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.0.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.0.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.1.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.1.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.1.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.1.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.1.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.1.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.2.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.2.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.2.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.2.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.2.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.2.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.3.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.3.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.3.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.3.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.3.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.3.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.4.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.4.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.4.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.4.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.4.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.4.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.5.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.5.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.5.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.5.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.5.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.5.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.6.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.6.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.6.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.6.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.6.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.6.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.7.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.7.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.7.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.7.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.7.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.7.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.8.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.8.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.8.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.8.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.8.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.8.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.9.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.9.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.9.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.9.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.9.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.9.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.10.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.10.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.10.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.10.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.10.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.10.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.11.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.11.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.11.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.11.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.11.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.11.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.12.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.12.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.12.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.12.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.12.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.12.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.13.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.13.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.13.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.13.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.13.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.13.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.14.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.14.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.14.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.14.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.14.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.14.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.15.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.15.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.15.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.15.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.15.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.15.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.16.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.16.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.16.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.16.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.16.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.16.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.17.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.17.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.17.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.17.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.17.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.17.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.18.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.18.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.18.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.18.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.18.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.18.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.19.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.19.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.19.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.19.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.19.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.19.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.20.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.20.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.20.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.20.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.20.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.20.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.21.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.21.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.21.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.21.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.21.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.21.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.22.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.22.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.22.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.22.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.22.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.22.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.23.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.23.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.23.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.23.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.23.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.23.fc2\n",
      "INFO:optimum.gptq.quantizer:Model packed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "自定义数据集量化模型生成结果:\n",
      "Merry Christmas! I'm glad to the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig\n",
    "import torch\n",
    "\n",
    "# 模型名称，可替换为\"facebook/opt-6.7b\"\n",
    "model_name_or_path = \"facebook/opt-1.3b\"\n",
    "\n",
    "# 配置GPTQ量化参数\n",
    "quantization_config = GPTQConfig(\n",
    "    bits=4,\n",
    "    group_size=128,\n",
    "    dataset=\"wikitext2\",\n",
    "    desc_act=False,\n",
    ")\n",
    "\n",
    "# 量化模型\n",
    "print(\"开始量化模型...\")\n",
    "quant_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# 检查量化正确性\n",
    "print(\"\\n检查量化是否成功：\")\n",
    "first_layer = quant_model.model.decoder.layers[0].self_attn.q_proj\n",
    "print(f\"量化权重(qweight)是否存在: {'qweight' in first_layer.__dict__}\")\n",
    "print(f\"量化零点(qzeros)数据类型: {first_layer.qzeros.dtype if 'qzeros' in first_layer.__dict__ else '未量化'}\")\n",
    "\n",
    "# 保存量化模型\n",
    "save_path = \"opt-1.3b-gptq-4bit\"\n",
    "quant_model.save_pretrained(save_path)\n",
    "print(f\"\\n量化模型已保存至: {save_path}\")\n",
    "\n",
    "# 加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 使用默认数据集量化的模型进行测试（加入指定文本）\n",
    "text = \"Merry Christmas! I'm glad to\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "\n",
    "out = quant_model.generate(** inputs, max_new_tokens=64)\n",
    "print(f\"\\n默认数据集量化模型生成结果:\")\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "\n",
    "# 自定义数据集量化演示\n",
    "custom_dataset = [\n",
    "    \"Quantization helps reduce model size significantly.\",\n",
    "    \"GPTQ is an efficient post-training quantization method for large language models.\"\n",
    "]\n",
    "\n",
    "custom_quant_config = GPTQConfig(\n",
    "    bits=4,\n",
    "    group_size=128,\n",
    "    desc_act=False,\n",
    "    dataset=custom_dataset\n",
    ")\n",
    "\n",
    "print(\"\\n使用自定义数据集进行量化...\")\n",
    "custom_quant_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    quantization_config=custom_quant_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# 使用自定义数据集量化的模型进行测试（加入指定文本）\n",
    "text = \"Merry Christmas! I'm glad to\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "\n",
    "out = custom_quant_model.generate(**inputs, max_new_tokens=64)\n",
    "print(f\"\\n自定义数据集量化模型生成结果:\")\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5fd04bf-71bc-4f91-96df-44870a68ca74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': True,\n",
       " '_parameters': {},\n",
       " '_buffers': {'qweight': tensor([[-1968072024, -1771280518,  1789289879,  ...,  1215579015,\n",
       "            2053435976, -1753460124],\n",
       "          [  967351451, -1785370714, -1778418564,  ...,  -878163915,\n",
       "            -110706971,  2105174792],\n",
       "          [  844485784, -1236563623,  -882554554,  ..., -1999525490,\n",
       "           -1734957961,  2020046922],\n",
       "          ...,\n",
       "          [ 1821035160, -1280943212, -1672110441,  ..., -1186514519,\n",
       "            1723308183, -1263167625],\n",
       "          [-1516574247, -1471756296, -1236731976,  ..., -2004788788,\n",
       "            1468696729,  1719643110],\n",
       "          [ -942189433, -1148750501, -1505130619,  ..., -1702438745,\n",
       "             -60205431,  -912684712]], device='cuda:0', dtype=torch.int32),\n",
       "  'qzeros': tensor([[-2004318072, -2004318072, -2004318072,  ..., -2004318072,\n",
       "           -2004318072, -2004318072],\n",
       "          [-2004318072, -2004318072, -2004318072,  ..., -2004318072,\n",
       "           -2004318072, -2004318072],\n",
       "          [-2004318072, -2004318072, -2004318072,  ..., -2004318072,\n",
       "           -2004318072, -2004318072],\n",
       "          ...,\n",
       "          [-2004318072, -2004318072, -2004318072,  ..., -2004318072,\n",
       "           -2004318072, -2004318072],\n",
       "          [-2004318072, -2004318072, -2004318072,  ..., -2004318072,\n",
       "           -2004318072, -2004318072],\n",
       "          [-2004318072, -2004318072, -2004318072,  ..., -2004318072,\n",
       "           -2004318072, -2004318072]], device='cuda:0', dtype=torch.int32),\n",
       "  'scales': tensor([[0.0029, 0.0037, 0.0039,  ..., 0.0035, 0.0028, 0.0038],\n",
       "          [0.0034, 0.0038, 0.0039,  ..., 0.0051, 0.0028, 0.0048],\n",
       "          [0.0040, 0.0030, 0.0041,  ..., 0.0050, 0.0043, 0.0036],\n",
       "          ...,\n",
       "          [0.0043, 0.0037, 0.0040,  ..., 0.0038, 0.0034, 0.0042],\n",
       "          [0.0034, 0.0035, 0.0043,  ..., 0.0048, 0.0033, 0.0034],\n",
       "          [0.0042, 0.0041, 0.0043,  ..., 0.0048, 0.0047, 0.0042]],\n",
       "         device='cuda:0', dtype=torch.float16),\n",
       "  'g_idx': tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0', dtype=torch.int32),\n",
       "  'bias': tensor([-0.0407, -0.0028, -0.0199,  ..., -0.0168, -0.0078, -0.0042],\n",
       "         device='cuda:0', dtype=torch.float16)},\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_pre_hooks': OrderedDict(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_hooks_with_kwargs': OrderedDict(),\n",
       " '_forward_hooks_always_called': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks_with_kwargs': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': {},\n",
       " 'name': 'gptqmodel.nn_modules.qlinear.tritonv2.TritonV2QuantLinear',\n",
       " 'in_features': 2048,\n",
       " 'out_features': 2048,\n",
       " 'group_size': 128,\n",
       " 'bits': 4,\n",
       " 'desc_act': False,\n",
       " 'pack_dtype': torch.int32,\n",
       " 'backend': <BACKEND.TRITON: 'triton'>,\n",
       " 'maxq': 15,\n",
       " 'adapter': None,\n",
       " 'optimized': True,\n",
       " 'pack_dtype_bits': 32,\n",
       " 'pack_np_dtype': numpy.int32,\n",
       " 'pack_np_math_dtype': numpy.uint32,\n",
       " 'pack_factor': 8,\n",
       " '_qzeros_format': 1,\n",
       " 'dequant_dtype': torch.int8,\n",
       " 'device': device(type='cuda', index=0),\n",
       " 'wf_unsqueeze_zero': tensor([[[ 0,  4,  8, 12, 16, 20, 24, 28]]], device='cuda:0',\n",
       "        dtype=torch.int32),\n",
       " 'wf_unsqueeze_neg_one': tensor([[[ 0],\n",
       "          [ 4],\n",
       "          [ 8],\n",
       "          [12],\n",
       "          [16],\n",
       "          [20],\n",
       "          [24],\n",
       "          [28]]], device='cuda:0', dtype=torch.int32),\n",
       " 'dequantize_weight': <function gptqmodel.nn_modules.qlinear.PackableQuantLinear.dequantize_weight(num_itr: int = 1)>}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_model.model.decoder.layers[0].self_attn.q_proj.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3c63bb5-83c3-48db-8ed5-8387aa81e482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merry Christmas! I'm glad to the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"Merry Christmas! I'm glad to\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "\n",
    "out = custom_quant_model.generate(**inputs, max_new_tokens=64)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dac21df-5362-4b07-a565-b14f7b7b801e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AliOpenAPI",
   "language": "python",
   "name": "aliopenapi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
